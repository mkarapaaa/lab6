import numpy as np

def f(x):
    x1, x2 = x
    return 7*x1**2 + 2*x1*x2 + 5*x2**2 + x1 - 10*x2

def grad_f(x):
    x1, x2 = x
    return np.array([14*x1 + 2*x2 + 1, 2*x1 - 10])

# Метод наискорейшего спуска
def gradient_descent(x0, alpha, epsilon, max_iter):
    x = x0
    iter_count = 0
    while True:
        grad = grad_f(x)
        if np.linalg.norm(grad) < epsilon:
            break
        x = x - alpha * grad
        iter_count += 1
        if iter_count > max_iter:
            break
    return x

x0 = np.array([1.0, 1.0])

# Параметры алгоритма
alpha = 0.01  # Шаг градиентного спуска
epsilon = 1e-6  # Точность сходимости
max_iter = 1000  # Максимальное число итераций

# Поиск оптимума
x_opt = gradient_descent(x0, alpha, epsilon, max_iter)
print("Оптимальное решение:", x_opt)
print("Значение функции в оптимальной точке:", f(x_opt))
